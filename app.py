import streamlit as st
import asyncio
import os
import shutil
import time
import zipfile
import io
import csv
from date_utils import parse_date
from download_api import download_vessel_track_data
from path_utils import get_output_dir_path

# --- é é¢è¨­å®š ---
st.set_page_config(page_title="èˆ¹èˆ¶è»Œè·¡ä¸‹è¼‰å™¨", layout="wide")
st.title("ğŸš¢ èˆ¹èˆ¶è»Œè·¡æ‰¹æ¬¡ä¸‹è¼‰ (MMSI / IMO)")

# --- å´é‚Šæ¬„ï¼šè¨­å®š ---
with st.sidebar:
    st.header("1. è¨­å®šåƒæ•¸")
    default_key = os.getenv("MARINE_TRAFFIC_API_KEY", "")
    api_key = st.text_input("API Key", value=default_key, type="password")
    
    col1, col2 = st.columns(2)
    start_date = col1.date_input("é–‹å§‹æ—¥æœŸ", value=parse_date("2023-01-01"))
    end_date = col2.date_input("çµæŸæ—¥æœŸ", value=parse_date("2023-01-05"))
    
    st.divider()
    st.header("2. ä¸‹è¼‰ç­–ç•¥")
    success_wait = st.number_input("æˆåŠŸå¾Œç­‰å¾… (ç§’)", value=60, min_value=0)
    fail_wait = st.number_input("å¤±æ•—å¾Œç­‰å¾… (ç§’)", value=20, min_value=0)

# --- ä¸»ç•«é¢ï¼šè¼¸å…¥èˆ‡è¼¸å‡º ---
col_left, col_right = st.columns([1, 1.5])

with col_left:
    st.subheader("è¼¸å…¥æ¸…å–®")
    raw_input = st.text_area("è«‹è¼¸å…¥ MMSI æˆ– IMO (ä¸€è¡Œä¸€å€‹)", height=300, 
                            placeholder="9123456\n416000000")
    start_btn = st.button("ğŸš€ é–‹å§‹æ‰¹æ¬¡ä¸‹è¼‰", use_container_width=True)

with col_right:
    st.subheader("åŸ·è¡Œé€²åº¦")
    status_box = st.container(border=True)
    progress_bar = status_box.progress(0)
    status_text = status_box.empty()
    log_area = st.empty()

# --- æ ¸å¿ƒé‚è¼¯ ---
async def run_batch_download():
    # 1. æº–å‚™è³‡æ–™
    id_list = [line.strip() for line in raw_input.split('\n') if line.strip()]
    if not id_list:
        st.error("âŒ è«‹è¼¸å…¥è‡³å°‘ä¸€çµ„ MMSI æˆ– IMO")
        return

    if not api_key:
        st.error("âŒ è«‹è¼¸å…¥ API Key")
        return

    temp_root = "./temp_web_download"
    # æ¸…ç©ºä¸¦é‡å»ºæš«å­˜ç›®éŒ„
    if os.path.exists(temp_root):
        shutil.rmtree(temp_root)
    os.makedirs(temp_root, exist_ok=True)

    total_ships = len(id_list)
    success_files = [] # ç´€éŒ„æˆåŠŸä¸‹è¼‰çš„æª”æ¡ˆè·¯å¾‘
    logs = []

    # 2. é–‹å§‹è¿´åœˆ
    for idx, vessel_id in enumerate(id_list):
        current = idx + 1
        progress_bar.progress(current / total_ships)
        status_text.markdown(f"### ğŸ”„ æ­£åœ¨è™•ç† ({current}/{total_ships}): `{vessel_id}`")
        
        # é¡¯ç¤º Log
        logs.insert(0, f"[{time.strftime('%H:%M:%S')}] é–‹å§‹ä¸‹è¼‰: {vessel_id}")
        log_area.text_area("åŸ·è¡Œæ—¥èªŒ", "\n".join(logs), height=250)

        # å‘¼å«ä¸‹è¼‰ API
        is_success = await download_vessel_track_data(
            api_key, vessel_id, start_date, end_date, temp_root
        )

        if is_success:
            logs.insert(0, f"âœ… {vessel_id} ä¸‹è¼‰æˆåŠŸï¼")
            
            # --- è‡ªå‹•åˆä½µ CSV (å°‡åˆ†æ®µæª”æ¡ˆåˆç‚ºä¸€å€‹) ---
            target_dir = get_output_dir_path(vessel_id, temp_root)
            if os.path.exists(target_dir):
                # æ‰¾å‡ºæ‰€æœ‰åˆ†æ®µ csv
                chunk_files = sorted([f for f in os.listdir(target_dir) if f.endswith(".csv")])
                if chunk_files:
                    combined_data = []
                    header = None
                    
                    for f in chunk_files:
                        with open(os.path.join(target_dir, f), 'r', encoding='utf-8') as cf:
                            reader = csv.reader(cf)
                            try:
                                h = next(reader)
                                if not header: header = h
                                for row in reader: combined_data.append(row)
                            except StopIteration: pass
                    
                    # å­˜æˆå–®ä¸€æª”æ¡ˆåˆ° temp_root æ ¹ç›®éŒ„ï¼Œæ–¹ä¾¿æ‰“åŒ…
                    final_filename = f"track_{vessel_id}.csv"
                    final_path = os.path.join(temp_root, final_filename)
                    with open(final_path, 'w', encoding='utf-8', newline='') as f:
                        writer = csv.writer(f)
                        if header: writer.writerow(header)
                        writer.writerows(combined_data)
                    
                    success_files.append(final_path)
            # ----------------------------------------

            wait_time = success_wait
        else:
            logs.insert(0, f"âš ï¸ {vessel_id} ä¸‹è¼‰å¤±æ•—æˆ–ç„¡è³‡æ–™ã€‚")
            wait_time = fail_wait

        log_area.text_area("åŸ·è¡Œæ—¥èªŒ", "\n".join(logs), height=250)

        # ç­‰å¾…æ©Ÿåˆ¶ (å¦‚æœæ˜¯æœ€å¾Œä¸€è‰˜å°±ä¸ç­‰)
        if current < total_ships:
            for t in range(wait_time, 0, -1):
                status_text.markdown(f"### â³ å†·å»ä¸­... å‰©é¤˜ {t} ç§’ (ä¸‹ä¸€è‰˜: {id_list[idx+1]})")
                time.sleep(1)

    # 3. å…¨éƒ¨å®Œæˆï¼Œæ‰“åŒ… ZIP
    status_text.success("ğŸ‰ æ‰€æœ‰ä»»å‹™åŸ·è¡Œå®Œç•¢ï¼")
    
    if success_files:
        # å»ºç«‹ ZIP æª”æ–¼è¨˜æ†¶é«”ä¸­
        zip_buffer = io.BytesIO()
        with zipfile.ZipFile(zip_buffer, "w") as zf:
            for file_path in success_files:
                # æŠŠæª”æ¡ˆåŠ å…¥ zipï¼Œä¸¦åªä¿ç•™æª”å (ä¸å«è·¯å¾‘)
                zf.write(file_path, arcname=os.path.basename(file_path))
        
        # é¡¯ç¤ºä¸‹è¼‰æŒ‰éˆ•
        st.balloons()
        st.download_button(
            label=f"ğŸ“¥ ä¸‹è¼‰ ZIP å£“ç¸®æª” (å…± {len(success_files)} å€‹æª”æ¡ˆ)",
            data=zip_buffer.getvalue(),
            file_name=f"vessel_tracks_{date.today()}.zip",
            mime="application/zip",
            use_container_width=True,
            type="primary"
        )
    else:
        st.warning("æ²’æœ‰æˆåŠŸä¸‹è¼‰ä»»ä½•æª”æ¡ˆã€‚")

# å•Ÿå‹•ç•°æ­¥ä»»å‹™
if start_btn:
    asyncio.run(run_batch_download())